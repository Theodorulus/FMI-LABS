# -*- coding: utf-8 -*-
"""lab6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yJn3JU36Ff5MLUmLHYz4GbRjgMLdQAET
"""

import numpy as np
from sklearn.utils import shuffle as shuffle_arrays

# antreneaza un perceptron folosind widrow-hoff
# returneaza W, b antrenate
# X_train - datele de antrenare
# y_train - etichete de antrenare
# shuffle - daca vrem sa amestecam datele la inceputul fiecarei epoci
# verbose - daca vrem sa afisam acuratetea pe multumea de antrenare dupa fiecare epoca

#1

def train(X_train, y_train, epochs = 5, learning_rate = 0.01, shuffle = True, verbose = True):
  W = np.zeros(X_train.shape[1])
  b = 0
  for i in range(epochs):
    sum_loss = 0
    if shuffle:
      X_train, y_train = shuffle_arrays(X_train, y_train, random_state = 0)
    for t in range(X_train.shape[0]):
      y_pred = X_train[t].dot(W) + b
      loss = (y_pred - y_train[t]) * (y_pred - y_train[t]) / 2
      W = W - learning_rate * (y_pred - y_train[t]) * X_train[t]
      b = b - learning_rate * (y_pred - y_train[t])

      sum_loss += loss
      
      if verbose:
        print(sum_loss)
  return W, b

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler

data = load_breast_cancer()
X_train, y_train = data.data, data.target

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
train(X_train, y_train, epochs = 10, learning_rate = 0.01, shuffle = True, verbose = True)

#2

import matplotlib.pyplot as plt
def compute_y(x, W, bias):
    # dreapta de decizie
    # [x, y] * [W[0], W[1]] + b = 0
    return (-x * W[0] - bias) / (W[1] + 1e-10)
def plot_decision_boundary(X, y , W, b, current_x, current_y):
    x1 = -0.5
    y1 = compute_y(x1, W, b)
    x2 = 0.5
    y2 = compute_y(x2, W, b)
    # sterge continutul ferestrei
    plt.clf()
    # ploteaza multimea de antrenare
    color = 'r'
    if current_y == -1:
        color = 'b'
    plt.ylim((-1, 2))
    plt.xlim((-1, 2))
    plt.plot(X[y == -1, 0], X[y == -1, 1], 'b+')
    plt.plot(X[y == 1, 0], X[y == 1, 1], 'r+')

    # ploteaza exemplul curent
    plt.plot(current_x[0], current_x[1], color+'s')

    # afisarea dreptei de decizie
    plt.plot([x1, x2] ,[y1, y2], 'black')
    plt.show(block=False)
    plt.pause(0.3)

def train(X_train, y_train,epochs = 70, learning_rate = 0.1, shuffle=True, verbose=True):
    W = np.zeros(X_train.shape[1])
    b = 0
    for i in range(epochs):
        sum_loss = 0
        if shuffle:
            X_train, y_train = shuffle_arrays(X_train, y_train, random_state=0)
        for t in range(X_train.shape[1]):
            y_pred = X_train[t].dot(W) + b
            loss = (y_pred - y_train[t]) * (y_pred - y_train[t]) / 2
            W = W - learning_rate * (y_pred - y_train[t]) * X_train[t]
            b = b - learning_rate * (y_pred - y_train[t])

            plot_decision_boundary(X_train, y_train ,W,b, X_train[t], y_train[t])

            sum_loss += loss

        if verbose:
            print (sum_loss)

    return W, b

X = np.array([ [0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([-1, 1, 1, 1])

train(X, y)

#3
miu = 0.0
sigma = 1.0
num_hidden_neurons = 5

W_1 = np.random.normal(miu, sigma, (2, num_hidden_neurons)) 
# generam aleator matricea ponderilor stratului ascuns (2 - dimensiunea datelor de intrare, num_hidden_neurons - numarul neuronilor de pe stratul ascuns), cu media miu si deviatia standard sigma. 
b_1 = np.zeros(num_hidden_neurons) 
# initializam bias-ul cu 0 
W_2 = np.random.normal(miu, sigma, (num_hidden_neurons, 1)) 
# generam aleator matricea ponderilor stratului de iesire (num_hidden_neurons - numarul neuronilor de pe stratul ascuns, 1 - un neuron pe stratul de iesire), cu media miu si deviatia standard sigma. 
b_2 = np.zeros(1) # initializam bias-ul cu 0

def signoid(x):
  return 1.0 / (1.0 + np.exp(-x))

def tanh_derivative(x):
  return 1.0 - np.tanh(x) * np.tanh(x)

def forward(X, W_1, b_1, W_2, b_2):
  z_1 = X.dot(W_1) + b_1
  a_1 = np.tanh(z_1)

  z_2 = x1 = a_1.dot(W_2) + b_2
  a_2 = signoid(z_2)

  return z_1, a_1, z_2, a_2

# forward(X, W_1, b_1, W_2, b_2) -> testare

def backward(a_1, a_2, z_1, W_2, X, Y, num_samples):
  dz_2 = a_2 - y # derivata functiei de pierdere (logistic loss) in functie de z 
  dw_2 = (a_1.T * dz_2) / num_samples # der(L/w_2) = der(L/z_2) * der(dz_2/w_2) = dz_2 * der((a_1 * W_2 + b_2)/ W_2) 
  db_2 = sum(dz_2) / num_samples # der(L/b_2) = der(L/z_2) * der(z_2/b_2) = dz_2 * der((a_1 * W_2 + b_2)/ b_2) # primul strat 
  da_1 = dz_2 * W_2.T # der(L/a_1) = der(L/z_2) * der(z_2/a_1) = dz_2 * der((a_1 * W_2 + b_2)/ a_1) 
  dz_1 = da_1 * tanh_derivative(z_1) # der(L/z_1) = der(L/a_1) * der(a_1/z1) = da_1 .* der((tanh(z_1))/ z_1) 
  dw_1 = X.T * dz_1 / num_samples # der(L/w_1) = der(L/z_1) * der(z_1/w_1) = dz_1 * der((X * W_1 + b_1)/ W_1) 
  db_1 = sum(dz_1) / num_samples # der(L/b_1) = der(L/z_1) * der(z_1/b_1) = dz_1 * der((X * W_1 + b_1)/ b_1) 
  return dw_1, db_1, dw_2, db_2

z_1, a_1, z_2, a_2 = forward(X, W_1, b_1, W_2, b_2)
loss = (-y * np.log(a_2) - (1 - y) * np.log(1 - a_2)).mean()
accuracy = (np.round(a_2) == y).mean()

#print(backward(a_1, a_2, z_1, W_2, X, y, len(X)))

print(accuracy)

def train_

def train(X_train, y_train,epochs = 70, learning_rate = 0.1, shuffle=True, verbose=True):
    W_1 = np.random.normal(miu, sigma, (2, num_hidden_neurons)) # generam aleator matricea ponderilor stratului ascuns (2 - dimensiunea datelor de intrare, num_hidden_neurons - numarul neuronilor de pe stratul ascuns), cu media miu si deviatia standard sigma. 
    b_1 = np.zeros(num_hidden_neurons) # initializam bias-ul cu 0 
    W_2 = np.random.normal(miu, sigma, (num_hidden_neurons, 1)) # generam aleator matricea ponderilor stratului de iesire (num_hidden_neurons - numarul neuronilor de pe stratul ascuns, 1 - un neuron pe stratul de iesire), cu media miu si deviatia standard sigma. 
    b_2 = np.zeros(1) # initializam bias-ul cu 0
    
    for i in range(epochs):
        sum_loss = 0
        if shuffle:
            X_train, y_train = shuffle_arrays(X_train, y_train, random_state=0)
        for t in range(X_train.shape[1]):
            y_pred = forward(X, W_1, b_1, W_2, b_2)
            loss = (-y * np.log(a_2) - (1 - y) * np.log(1 - a_2)).mean()
            
            dw1, db1, dw2, db2 = backward(a_1, a_2, z_1, W_2, X_train, y_train, len(X_train))

            W_1 = W_1 - learning_rate * dw1

    return W_1, b_1, W_2, b_2