# -*- coding: utf-8 -*-
"""lab5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bHnWEHrU01ZM_KbYpm4mz3ONOqaEZi-C
"""

!unzip data_lab6.zip

import numpy as np 
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

# load training data 
training_data = np.load('data/training_data.npy') 
prices = np.load('data/prices.npy') 

# print the first 4 samples 
print('The first 4 samples are:\n ', training_data[:4]) 
print('The first 4 prices are:\n ', prices[:4]) 

# shuffle 
training_data, prices = shuffle(training_data, prices, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(training_data, prices, test_size=0.2, random_state=42)

#1 + tipul normalizarii
import sklearn.preprocessing as preprocessing

def normalize_data(train_data, test_data, type='standard'):
    if type == 'standard':
        scaler = preprocessing.StandardScaler()
    elif type == 'minmax':
        scaler = preprocessing.MinMaxScaler()
    
    scaler.fit(train_data)
    sc_train = scaler.transform(train_data)
    sc_test = scaler.transform(test_data)
    return sc_train, sc_test

normalize_data(X_train, X_test, 'minmax')

#2 + nenormalizat, dar cu 5 fold-uri

from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import cross_validate

lin_model = LinearRegression()
result = cross_validate(lin_model, X_train, y_train, scoring={
                                'MSE': make_scorer(mean_squared_error),
                                'MAE': make_scorer(mean_absolute_error),
                            },
                         cv=5, n_jobs=-1, verbose=10)
print (np.mean(result['test_MSE']), np.mean(result['test_MAE']))

X_train_norm, X_test_norm = normalize_data(X_train, X_test)

lin_model = LinearRegression()
result = cross_validate(lin_model, X_train_norm, y_train, scoring={
                                'MSE': make_scorer(mean_squared_error),
                                'MAE': make_scorer(mean_absolute_error),
                            },
                         cv=5, n_jobs=-1, verbose=10)

print (np.mean(result['test_MSE']), np.mean(result['test_MAE']))

#3 + model ridge, lasso + ambele tipuri de normalizari

for method in ["standard","minmax"]:
    for alpha in [1, 10, 100, 1000]:
        X_train_norm, X_test_norm = normalize_data(X_train, X_test, method)

        lin_model = Ridge(alpha)
        result = cross_validate(lin_model, X_train_norm, y_train, scoring={
                                        'MSE': make_scorer(mean_squared_error),
                                        'MAE': make_scorer(mean_absolute_error),
                                    },
                                 cv=5, n_jobs=-1)

        print (method, np.mean(result['test_MSE']), np.mean(result['test_MAE']))

print('---------------------------------------------')

for method in ["standard","minmax"]:
    for alpha in [1, 10, 100, 1000]:
        X_train_norm, X_test_norm = normalize_data(X_train, X_test, method)

        lin_model = Lasso(alpha)
        result = cross_validate(lin_model, X_train_norm, y_train, scoring={
                                        'MSE': make_scorer(mean_squared_error),
                                        'MAE': make_scorer(mean_absolute_error),
                                    },
                                 cv=5, n_jobs=-1)

        print (method, np.mean(result['test_MSE']), np.mean(result['test_MAE']))

#4 
from sklearn.linear_model import Ridge, Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from itertools import product
from sklearn.model_selection import cross_val_score

pipeline_grid = {
    'scale': [MinMaxScaler(), StandardScaler()],
    'train': [Ridge(), Lasso()],
}
steps = ['scale', 'train']

cv_grid = {
    'train__alpha': [1, 10, 100, 1000]
}

def create_pipelines(pipeline_grid, steps):
    combs = product(*[pipeline_grid[k] for k in steps])
    args = [zip(steps, comb) for comb in combs]
    pipelines = [Pipeline(list(arg)) for arg in args]
    return pipelines

for p in create_pipelines(pipeline_grid, steps):
    res = GridSearchCV(p, param_grid=cv_grid, scoring='neg_mean_squared_error', 
                                cv=3, n_jobs=-1, verbose=0)
    res.fit(X_train, y_train)
    
    print (f"Variant {p}: {res.best_score_} {res.best_params_}")
    model = res.best_estimator_['train']
    print (f"Largetst coef: {np.argmax(model.coef_)}")
    print (f"Smallest coef: {np.argmin(model.coef_)}")
    
    cnt_0 = len([x for x in model.coef_ if x < 1e-3])
    print (f"Zero coefs: {cnt_0}")