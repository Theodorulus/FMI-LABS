# -*- coding: utf-8 -*-
"""LAB4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CG5pYhG53BG-8aDwhNQVdXUFKbtO2UdE
"""

!unzip data_lab5.zip

def normalize_data(train_data, test_data, type=None):
  if type == 'standard':
    scaler = preprocessing.StandardScaler()
    scaler.fit(train_data)
    scaled_train_data = scaler.transform(train_data)
    scaled_test_data = scaler.transform(test_data)

  if type == 'l1':
    scaler = preprocessing.Normalizer(norm='l1')
    scaler.fit(train_data)
    scaled_train_data = scaler.transform(train_data)
    scaled_test_data = scaler.transform(test_data)

  if type == 'l2':
    scaler = preprocessing.Normalizer(norm='l2')
    scaler.fit(train_data)
    scaled_train_data = scaler.transform(train_data)
    scaled_test_data = scaler.transform(test_data)

  else:
    scaled_train_data = train_data
    scaled_test_data = test_data

  return scaled_train_data, scaled_test_data

class BagOfWords:

    def __init__(self):
        self.vocabular = {}
        self.cuvinte = []
        self.id = 0

    def build_vocabulary (self, data):
        for eseu in data:
            for cuvant in eseu:
                if cuvant not in self.vocabular.keys():
                    self.vocabular[cuvant] = self.id
                    self.id += 1
                    self.cuvinte.append(cuvant)

    def get_features (self, data):
        features = np.zeros((len(data), self.id))
        # id_sample = -1
        for id_sample, sample in enumerate(data):
            # id_sample += 1
            for word in sample:
                if word in self.vocabular.keys():
                    features[id_sample][self.vocabular[word]] += 1
        return features

train_data = np.load('data/training_sentences.npy', allow_pickle=True)
train_labels = np.load('data/training_labels.npy', allow_pickle=True)

test_data = np.load('data/test_sentences.npy', allow_pickle=True)
test_labels = np.load('data/test_labels.npy', allow_pickle=True)

train_labels

bow = BagOfWords()
bow.build_vocabulary(train_data)
train = bow.get_features(train_data)
test = bow.get_features(test_data)
normalized_train, normalized_test = normalize_data(train, test, 'l2')

print(normalized_train)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score


# instantiate SVM
model = SVC(C=1, kernel='linear')
# fit SVM
model.fit(normalized_train, train_labels)
# get predictions SVM
predict = model.predict(normalized_test)
# calculate accuracy/f1
print(accuracy_score(test_labels, predict))
print(f1_score(test_labels, predict))

model.coef_[0]

idxes = np.argsort(model.coef_[0])

print(idxes.shape)

print(np.min(model.coef_[0]))
print(model.coef_[0][idxes[0]])

print([bow.cuvinte[i] for i in idxes[:10]])

print([bow.cuvinte[i] for i in idxes[-10:]])

class BagOfWords:

    def __init__(self):
        self.vocabular = {}
        self.cuvinte = []
        self.id = 0

    def build_vocabulary(self, data, vocab_size=5000):
        for eseu in data:
            for cuvant in eseu:
                if cuvant not in self.vocabular.keys():
                    self.vocabular[cuvant] = self.id
                    self.id += 1
                    self.cuvinte.append(cuvant)

    def get_features (self, data):
        features = np.zeros((len(data), self.id))
        # id_sample = -1
        for id_sample, sample in enumerate(data):
            # id_sample += 1
            for word in sample:
                if word in self.vocabular.keys():
                    features[id_sample][self.vocabular[word]] += 1
        return features











import numpy as np

train_data = np.load('data/training_sentences.npy', allow_pickle=True)
train_labels = np.load('data/training_labels.npy', allow_pickle=True)

test_data = np.load('data/test_sentences.npy', allow_pickle=True)
test_labels = np.load('data/test_labels.npy', allow_pickle=True)

X_train, y_train = np.load('data/training_sentences.npy', allow_pickle=True), np.load('data/training_labels.npy', allow_pickle=True)
X_test, y_test = np.load('data/test_sentences.npy', allow_pickle=True), np.load('data/test_labels.npy', allow_pickle=True)


print(X_train.shape)
print(X_test.shape)

import sklearn.preprocessing as preprocessing

#2
def normalize_data(train_data, test_data, type=None):
    if type == 'standard':
        scaler = preprocessing.StandardScaler()
    elif type == 'l1':
        scaler = preprocessing.Normalizer(norm='l1')
    elif type == 'l2':
        scaler = preprocessing.Normalizer(norm='l2')

    if scaler is not None:
        scaler.fit(train_data)
        sc_train = scaler.transform(train_data)
        sc_test = scaler.transform(test_data)
        return sc_train, sc_test
    return train_data, test_data

def normalize_data(train_data, test_data, type = None):
    n_train_data = train_data
    n_test_data = test_data
    if type == 'standard':
        scaler = preprocessing.StandardScaler()
        scaler.fit(train_data)
        n_train_data = scaler.transform(train_data)
        n_test_data = scaler.transform(test_data)
    elif type == 'l1':
        x_norm = np.sum(abs(train_data), axis = 1)
        n_train_data = train_data/x_norm
        n_test_data = test_data/x_norm
    elif type == 'l2':
        x_norm = np.sqrt(np.sum((train_data ** 2), axis = 1))
        # print(np.sqrt(np.sum((train_data ** 2), axis = 1)).shape)
        # print(train_data.shape)

        # print(np.sqrt(np.sum((test_data ** 2), axis = 1)).shape)
        # print(test_data.shape)
        n_train_data = train_data / np.sqrt(np.sum((train_data ** 2), axis = 1).reshape(3734, 1))
        n_test_data = test_data / np.sqrt(np.sum((test_data ** 2), axis = 1).reshape(1840, 1))
    return n_train_data, n_test_data

normalize_data(X_train, X_test, type='l2')

class BagOfWords():
    def __init__(self):
        self.words = []
        self.word_to_id = {}
        
    def build_vocabulary(self, data):
        for sentence in data:
            for word in sentence:
                if word not in self.word_to_id:
                    self.word_to_id[word] = len(self.words)
                    self.words.append(word)
                    
    def get_features(self, data):
        # features = np.array([[0 for _ in range(len(self.words))] for _ in range(len(data))])
        features = np.zeros((len(data), len(self.words)))
        for sentence_id, sentence in enumerate(data):
            for word in sentence:
                if word in self.word_to_id:
                    features[sentence_id][self.word_to_id[word]] += 1
        return features
        
bow = BagOfWords()
bow.build_vocabulary(X_train)
X_train = bow.get_features(X_train)
X_test  = bow.get_features(X_test)

train_normalised, test_normalised = normalize_data(X_train, X_test, type='l2')

print(train_normalised[0])

import sklearn.svm as svm
from sklearn.metrics import accuracy_score, f1_score

vect_class = svm.SVC(C=1, kernel='linear')
vect_class.fit(X_train, train_labels)

test_predict = vect_class.predict(X_test)
print('Accuracy: ', accuracy_score(test_labels, test_predict))

print('F1 score: ', f1_score(test_labels, test_predict))

vect_class.coef_.shape

print(np.min(vect_class.coef_))

sc = np.argsort(vect_class.coef_)[0]

print(sc.shape)

print(sc[0])
print(vect_class.coef_[0, sc[0]])

top_negative_words = [bow.words[i] for i in sc[:10]]
print(top_negative_words)

# NOT SPAM

top_positive_words = [bow.words[i] for i in sc[-10:]]
print(top_positive_words)

# SPAM

