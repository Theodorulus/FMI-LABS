# -*- coding: utf-8 -*-
"""lab4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14T34kF3YKy4jt1HB8pRNEtZ9RVdDMlQZ
"""

!unzip data_lab5.zip

import numpy as np
import sklearn.preprocessing as preprocessing

train_data = np.load('data/training_sentences.npy', allow_pickle=True)
train_labels = np.load('data/training_labels.npy', allow_pickle=True)

test_data = np.load('data/test_sentences.npy', allow_pickle=True)
test_labels = np.load('data/test_labels.npy', allow_pickle=True)

X_train, y_train = np.load('data/training_sentences.npy', allow_pickle=True), np.load('data/training_labels.npy', allow_pickle=True)
X_test, y_test = np.load('data/training_sentences.npy', allow_pickle=True), np.load('data/training_labels.npy', allow_pickle=True)

#2
def normalize_data(train_data, test_data, type=None):
    if type == 'standard':
        scaler = preprocessing.StandardScaler()
    elif type == 'l1':
        scaler = preprocessing.Normalizer(norm='l1')
    elif type == 'l2':
        scaler = preprocessing.Normalizer(norm='l2')

    if scaler is not None:
        scaler.fit(train_data)
        sc_train = scaler.transform(train_data)
        sc_test = scaler.transform(test_data)
        return sc_train, sc_test
    return train_data, test_data

#3
class BagOfWords():
    def __init__(self):
        self.words = []
        self.word_to_id = {}
        
    def build_vocabulary(self, data):
        for sentence in data:
            for word in sentence:
                if word not in self.word_to_id:
                    self.word_to_id[word] = len(self.words)
                    self.words.append(word)
    #4                
    def get_features(self, data):
        features = np.array([[0 for _ in range(len(self.words))] for _ in range(len(data))])
        for sentence_id, sentence in enumerate(data):
            for word in sentence:
                if word in self.word_to_id:
                    features[sentence_id][self.word_to_id[word]] += 1
        return features
        
bow = BagOfWords()
bow.build_vocabulary(X_train)
X_train = bow.get_features(X_train)
X_test  = bow.get_features(X_test)

#5
train_normalised, test_normalised = normalize_data(X_train, X_test, type = 'l2')

print(train_normalised[0])

#6 -> nu merge ???
from sklearn import svm
from sklearn.metrics import accuracy_score, f1_score
vect_class = svm.SVC(C=1, kernel='linear')
vect_class.fit(train_normalised, train_labels)

test_predict = vect_class.predict(test_normalised)
print('Accuracy: ', accuracy_score(test_labels, test_predict))

print('F1 score: ', f1_score(test_labels, test_predict))

#6 b)
sc = np.argsort(vect_class.coef_)[0]
print(vect_class.coef_[0, sc[0]])

top_negative_word_idxes = vect_class.coef_[0, sc[:10]]
print(top_negative_word_idxes)

top_negative_words = [bow.words[i] for i in sc[:10]]
print(top_negative_words)

#NOT SPAM

top_positive_words = [bow.words[i] for i in sc[-10:]]
print(top_positive_words)

#SPAM